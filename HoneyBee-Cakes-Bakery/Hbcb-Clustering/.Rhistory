ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE)
.libPaths()
lapply(.libPaths(), list.files)
if (require("languageserver")) {
require("languageserver")
} else {
install.packages("languageserver", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
# STEP 1. Install and Load the Required Packages ----
## readr ----
if (require("readr")) {
require("readr")
} else {
install.packages("readr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## naniar ----
if (require("naniar")) {
require("naniar")
} else {
install.packages("naniar", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggplot2 ----
if (require("ggplot2")) {
require("ggplot2")
} else {
install.packages("ggplot2", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## corrplot ----
if (require("corrplot")) {
require("corrplot")
} else {
install.packages("corrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggcorrplot ----
if (require("ggcorrplot")) {
require("ggcorrplot")
} else {
install.packages("ggcorrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## caret ----
if (require("caret")) {
require("caret")
} else {
install.packages("caret", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## tidyverse ----
if (require("tidyverse")) {
require("tidyverse")
} else {
install.packages("tidyverse", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
library(readr)
Hbcb_training_data <- read_csv("Hbcb-training-data.csv")
View(Hbcb_training_data)
str(Hbcb_training_data)
dim(Hbcb_training_data)
head(Hbcb_training_data)
summary(Hbcb_training_data)
# STEP 3. Check for Missing Data and Address it ----
# Are there missing values in the dataset?
any_na(Hbcb_training_data)
# How many?
n_miss(Hbcb_training_data)
# What is the proportion of missing data in the entire dataset?
prop_miss(Hbcb_training_data)
# What is the number and percentage of missing values grouped by
# each variable?
miss_var_summary(Hbcb_training_data)
# Which variables contain the most missing values?
gg_miss_var(Hbcb_training_data)
# Which combinations of variables are missing together?
gg_miss_upset(Hbcb_training_data)
# Where are missing values located (the shaded regions in the plot)?
vis_miss(Hbcb_training_data) +
theme(axis.text.x = element_text(angle = 80))
# Create a correlation matrix
# Option 1: Basic Table
cor(Hbcb_training_data[, c(3, 4)]) %>%
View()
# Option 2: Basic Plot
cor(Hbcb_training_data[, c(3, 4)]) %>%
corrplot(method = "square")
# Option 3: Fancy Plot using ggplot2
corr_matrix <- cor(Hbcb_training_data[, c(3, 4)])
p <- ggplot2::ggplot(data = reshape2::melt(corr_matrix),
ggplot2::aes(Var1, Var2, fill = value)) +
ggplot2::geom_tile() +
ggplot2::geom_text(ggplot2::aes(label = label_wrap(label, width = 10)),
size = 4) +
ggplot2::theme_minimal() +
ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE)
## Plot the scatter plots ----
# A scatter plot to show Phenols against Flavanoids
ggplot(Hbcb_training_data, aes(x=productId, y=quantity)) +
geom_point() +
geom_smooth(method="lm", se=FALSE) +
labs(title="Cake analysis",
subtitle="Relationship between Type of cake and quantity") +
theme_bw()
summary(Hbcb_training_data)
model_of_the_transform_Hbcb_training_data <- preProcess(Hbcb_training_data,
method = c("scale", "center"))
print(model_of_the_transform_Hbcb_training_data)
Hbcb_training_data_std <- predict(model_of_the_transform_Hbcb_training_data, Hbcb_training_data)
summary(Hbcb_training_data_std)
summary(Hbcb_training_data_std)
sapply(Hbcb_training_data_std[, c(1, 2, 3, 4, 5, 6)], sd)
Hbcb_training_data_vars <-
Hbcb_training_data_std[, c("productId","quantity")]
# STEP 5. Create the clusters using the K-Means Clustering Algorithm ----
# We start with a random guess of the number of clusters we need
set.seed(7)
kmeans_cluster <- kmeans(Hbcb_training_data_vars, centers = 2, nstart = 20)
# We then decide the maximum number of clusters to investigate
n_clusters <- 4
# Initialize total within sum of squares error: wss
wss <- numeric(n_clusters)
set.seed(7)
# Investigate 1 to n possible clusters (where n is the maximum number of
# clusters that we want to investigate)
for (i in 1:n_clusters) {
# Use the K Means cluster algorithm to create each cluster
kmeans_cluster <- kmeans(Hbcb_training_data_vars, centers = i, nstart = 20)
# Save the within cluster sum of squares
wss[i] <- kmeans_cluster$tot.withinss
}
## Plot a scree plot ----
# The scree plot should help you to note when additional clusters do not make
# any significant difference (the plateau).
wss_df <- tibble(clusters = 1:n_clusters, wss = wss)
scree_plot <- ggplot(wss_df, aes(x = clusters, y = wss, group = 1)) +
geom_point(size = 4) +
geom_line() +
scale_x_continuous(breaks = c(2, 4, 6, 8)) +
xlab("Number of Clusters")
scree_plot
# We can add guides to make it easier to identify the plateau (or "elbow").
scree_plot +
geom_hline(
yintercept = wss,
linetype = "dashed",
col = "red"  # Set a single color, e.g., black
)
# The plateau is reached at 6 clusters.
# We therefore create the final cluster with 6 clusters
# (not the initial 3 used at the beginning of this STEP.)
k <- 4
set.seed(7)
# Build model with k clusters: kmeans_cluster
kmeans_cluster <- kmeans(Hbcb_training_data_vars, centers = k, nstart = 20)
# STEP 6. Add the cluster number as a label for each observation ----
Hbcb_training_data_vars$cluster_id <- factor(kmeans_cluster$cluster)
## View the results by plotting scatter plots with the labelled cluster ----
ggplot(Hbcb_training_data_vars, aes(productId, quantity,
color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("productId") +
ylab("quantity")
summary(Hbcb_training_data)
model_of_the_transform_Hbcb_training_data <- preProcess(Hbcb_training_data,
method = c("scale", "center"))
print(model_of_the_transform_Hbcb_training_data)
Hbcb_training_data_std <- predict(model_of_the_transform_Hbcb_training_data, Hbcb_training_data)
summary(Hbcb_training_data_std)
sapply(Hbcb_training_data_std[, c(1, 2, 3, 4, 5, 6)], sd)
Hbcb_training_data_vars <-
Hbcb_training_data_std[, c("productId","quantity")]
# STEP 5. Create the clusters using the K-Means Clustering Algorithm ----
# We start with a random guess of the number of clusters we need
set.seed(7)
kmeans_cluster <- kmeans(Hbcb_training_data_vars, centers = 2, nstart = 20)
# We then decide the maximum number of clusters to investigate
n_clusters <- 4
# Initialize total within sum of squares error: wss
wss <- numeric(n_clusters)
set.seed(7)
# Investigate 1 to n possible clusters (where n is the maximum number of
# clusters that we want to investigate)
for (i in 1:4_clusters) {
## Plot a scree plot ----
# The scree plot should help you to note when additional clusters do not make
# any significant difference (the plateau).
wss_df <- tibble(clusters = 1:4_clusters, wss = wss)
scree_plot <- ggplot(wss_df, aes(x = clusters, y = wss, group = 1)) +
geom_point(size = 4) +
geom_line() +
scale_x_continuous(breaks = c(2, 4, 6, 8)) +
xlab("Number of Clusters")
scree_plot
# We can add guides to make it easier to identify the plateau (or "elbow").
scree_plot +
geom_hline(
yintercept = wss,
linetype = "dashed",
col = "red"  # Set a single color, e.g., black
)
# The plateau is reached at 6 clusters.
# We therefore create the final cluster with 6 clusters
# (not the initial 3 used at the beginning of this STEP.)
k <- 4
set.seed(7)
# Build model with k clusters: kmeans_cluster
kmeans_cluster <- kmeans(Hbcb_training_data_vars, centers = k, nstart = 20)
# STEP 6. Add the cluster number as a label for each observation ----
Hbcb_training_data_vars$cluster_id <- factor(kmeans_cluster$cluster)
## View the results by plotting scatter plots with the labelled cluster ----
ggplot(Hbcb_training_data_vars, aes(productId, quantity,
color = cluster_id)) +
geom_point(alpha = 1) +
xlab("productId") +
ylab("quantity")
## Plot the scatter plots ----
# A scatter plot to show Phenols against Flavanoids
ggplot(Hbcb_training_data, aes(x=productId, y=quantity)) +
geom_point() +
geom_smooth(method="lm", se=FALSE) +
labs(title="Cake analysis",
subtitle="Relationship between Type of cake and quantity") +
theme_bw()
# Select relevant columns for clustering
data_for_clustering <- Hbcb_training_data[, c("productId", "quantity")]
# Standardize the data (optional but often recommended for k-means)
scaled_data <- scale(data_for_clustering)
# Perform k-means clustering with 4 clusters
k <- 4  # Replace with the desired number of clusters
kmeans_result <- kmeans(scaled_data, centers = k)
# Add the cluster assignments back to the original data frame
Hbcb_training_data$cluster <- kmeans_result$cluster
# Print the results
print(kmeans_result)
print(Hbcb_training_data)
.libPaths()
lapply(.libPaths(), list.files)
if (require("languageserver")) {
require("languageserver")
} else {
install.packages("languageserver", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
# STEP 1. Install and Load the Required Packages ----
## readr ----
if (require("readr")) {
require("readr")
} else {
install.packages("readr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## naniar ----
if (require("naniar")) {
require("naniar")
} else {
install.packages("naniar", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggplot2 ----
if (require("ggplot2")) {
require("ggplot2")
} else {
install.packages("ggplot2", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## corrplot ----
if (require("corrplot")) {
require("corrplot")
} else {
install.packages("corrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggcorrplot ----
if (require("ggcorrplot")) {
require("ggcorrplot")
} else {
install.packages("ggcorrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## caret ----
if (require("caret")) {
require("caret")
} else {
install.packages("caret", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## tidyverse ----
if (require("tidyverse")) {
require("tidyverse")
} else {
install.packages("tidyverse", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
library(readr)
Hbcb_training_data <- read_csv("Hbcb-training-data.csv")
View(Hbcb_training_data)
str(Hbcb_training_data)
dim(Hbcb_training_data)
head(Hbcb_training_data)
summary(Hbcb_training_data)
# STEP 3. Check for Missing Data and Address it ----
# Are there missing values in the dataset?
any_na(Hbcb_training_data)
# How many?
n_miss(Hbcb_training_data)
# What is the proportion of missing data in the entire dataset?
prop_miss(Hbcb_training_data)
# What is the number and percentage of missing values grouped by
# each variable?
miss_var_summary(Hbcb_training_data)
# Which variables contain the most missing values?
gg_miss_var(Hbcb_training_data)
# Which combinations of variables are missing together?
gg_miss_upset(Hbcb_training_data)
# Where are missing values located (the shaded regions in the plot)?
vis_miss(Hbcb_training_data) +
theme(axis.text.x = element_text(angle = 80))
# Create a correlation matrix
# Option 1: Basic Table
cor(Hbcb_training_data[, c(3, 4)]) %>%
View()
# Option 2: Basic Plot
cor(Hbcb_training_data[, c(3, 4)]) %>%
corrplot(method = "square")
# Option 3: Fancy Plot using ggplot2
corr_matrix <- cor(Hbcb_training_data[, c(3, 4)])
p <- ggplot2::ggplot(data = reshape2::melt(corr_matrix),
ggplot2::aes(Var1, Var2, fill = value)) +
ggplot2::geom_tile() +
ggplot2::geom_text(ggplot2::aes(label = label_wrap(label, width = 10)),
size = 4) +
ggplot2::theme_minimal() +
ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE)
# Select relevant columns for clustering
data_for_clustering <- Hbcb_training_data[, c("productId", "quantity")]
# Standardize the data (optional but often recommended for k-means)
scaled_data <- scale(data_for_clustering)
# Perform k-means clustering with 4 clusters
k <- 4  # Replace with the desired number of clusters
kmeans_result <- kmeans(scaled_data, centers = k)
# Add the cluster assignments back to the original data frame
Hbcb_training_data$cluster <- kmeans_result$cluster
# Print the results
print(kmeans_result)
print(Hbcb_training_data)
summary(Hbcb_training_data)
model_of_the_transform_Hbcb_training_data <- preProcess(Hbcb_training_data,
method = c("scale", "center"))
## View the results by plotting scatter plots with the labelled cluster ----
ggplot(Hbcb_training_data, aes(productId, quantity,
color = cluster_id)) +
geom_point(alpha = 1) +
xlab("productId") +
ylab("quantity")
# STEP 6. Add the cluster number as a label for each observation ----
Hbcb_training_data$cluster_id <- factor(kmeans_cluster$cluster)
# Build model with k clusters: kmeans_cluster
kmeans_cluster <- kmeans(Hbcb_training_data, centers = k, nstart = 20)
# STEP 6. Add the cluster number as a label for each observation ----
Hbcb_training_data$cluster_id <- factor(kmeans_cluster$cluster)
## View the results by plotting scatter plots with the labelled cluster ----
ggplot(Hbcb_training_data, aes(productId, quantity,
color = cluster_id)) +
geom_point(alpha = 1) +
xlab("productId") +
ylab("quantity")
## View the results by plotting scatter plots with the labelled cluster ----
ggplot(Hbcb_training_data, aes(productId, quantity,)) +
geom_point(alpha = 1) +
xlab("productId") +
ylab("quantity")
# Create a scatter plot with different colors for each cluster
ggplot(Hbcb_training_data, aes(x = productId, y = quantity, color = factor(cluster))) +
geom_point() +
labs(title = "K-Means Clustering",
x = "Product ID",
y = "Quantity") +
theme_minimal()
.libPaths()
lapply(.libPaths(), list.files)
if (require("languageserver")) {
require("languageserver")
} else {
install.packages("languageserver", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
# STEP 1. Install and Load the Required Packages ----
## readr ----
if (require("readr")) {
require("readr")
} else {
install.packages("readr", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## naniar ----
if (require("naniar")) {
require("naniar")
} else {
install.packages("naniar", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggplot2 ----
if (require("ggplot2")) {
require("ggplot2")
} else {
install.packages("ggplot2", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## corrplot ----
if (require("corrplot")) {
require("corrplot")
} else {
install.packages("corrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## ggcorrplot ----
if (require("ggcorrplot")) {
require("ggcorrplot")
} else {
install.packages("ggcorrplot", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## caret ----
if (require("caret")) {
require("caret")
} else {
install.packages("caret", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
## tidyverse ----
if (require("tidyverse")) {
require("tidyverse")
} else {
install.packages("tidyverse", dependencies = TRUE,
repos = "https://cloud.r-project.org")
}
library(readr)
Hbcb_training_data <- read_csv("Hbcb-training-data.csv")
View(Hbcb_training_data)
str(Hbcb_training_data)
dim(Hbcb_training_data)
head(Hbcb_training_data)
summary(Hbcb_training_data)
# STEP 3. Check for Missing Data and Address it ----
# Are there missing values in the dataset?
any_na(Hbcb_training_data)
# How many?
n_miss(Hbcb_training_data)
# What is the proportion of missing data in the entire dataset?
prop_miss(Hbcb_training_data)
# What is the number and percentage of missing values grouped by
# each variable?
miss_var_summary(Hbcb_training_data)
# Which variables contain the most missing values?
gg_miss_var(Hbcb_training_data)
# Which combinations of variables are missing together?
gg_miss_upset(Hbcb_training_data)
# Where are missing values located (the shaded regions in the plot)?
vis_miss(Hbcb_training_data) +
theme(axis.text.x = element_text(angle = 80))
# Create a correlation matrix
# Option 1: Basic Table
cor(Hbcb_training_data[, c(3, 4)]) %>%
View()
# Option 2: Basic Plot
cor(Hbcb_training_data[, c(3, 4)]) %>%
corrplot(method = "square")
# Option 3: Fancy Plot using ggplot2
corr_matrix <- cor(Hbcb_training_data[, c(3, 4)])
p <- ggplot2::ggplot(data = reshape2::melt(corr_matrix),
ggplot2::aes(Var1, Var2, fill = value)) +
ggplot2::geom_tile() +
ggplot2::geom_text(ggplot2::aes(label = label_wrap(label, width = 10)),
size = 4) +
ggplot2::theme_minimal() +
ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE)
## Plot the scatter plots ----
# A scatter plot to show Cake type against quantity
ggplot(Hbcb_training_data, aes(x=productId, y=quantity)) +
geom_point() +
geom_smooth(method="lm", se=FALSE) +
labs(title="Cake analysis",
subtitle="Relationship between Type of cake and quantity") +
theme_bw()
#STEP 5. Training the Model
# Select relevant columns for clustering
data_for_clustering <- Hbcb_training_data[, c("productId", "quantity")]
# Standardize the data (optional but often recommended for k-means)
scaled_data <- scale(data_for_clustering)
# Perform k-means clustering with 4 clusters
k <- 4  # Replace with the desired number of clusters
kmeans_result <- kmeans(scaled_data, centers = k)
# Add the cluster assignments back to the original data frame
Hbcb_training_data$cluster <- kmeans_result$cluster
# Print the results
print(kmeans_result)
print(Hbcb_training_data)
# Create a scatter plot with different colors for each cluster
ggplot(Hbcb_training_data, aes(x = productId, y = quantity, color = factor(cluster))) +
geom_point() +
labs(title = "K-Means Clustering",
x = "Product ID",
y = "Quantity") +
theme_minimal()
